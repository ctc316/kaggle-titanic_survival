{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citation: \n",
    "#Code in this file has use Rishabh Misra's Logistic Regression code as reference. \n",
    "#link to the reference code: https://www.kaggle.com/rmisra/logistic-regression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model.logistic as logis\n",
    "import csv\n",
    "\n",
    "#load data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose features\n",
    "## Q: Using logistic regression, try to predict whether a passenger survived the disaster. You can choose the features (or combinations of features) you would like to use or ignore, provided you justify your reasoning.\n",
    "\n",
    "### We want to use feature data to predict whether a passenger survive in the disaster. Therefore, we only want to use features which related to survival rate in disaster. \n",
    "### First, we know that ticket class and ticket fare is crucial. People who are in the first class or pay a lot to buy ticket are often the more powerful people. Age and gender are also very important in this event because we know from saying that people follow \"Women and Children First\" policy in this event. \n",
    "### Ticket number, passenger id and name is totally irrelevant in this question. All three features only represent ways to identify a person. Pork of embarkation is also irrelevant in this question because the disaster happended after all people embarked. Cabin data is useless because there are too many NaN values. \n",
    "### There are two family related data. One is number of siblings/spouses aboard the Titanic, and the other one is number of parents/children aborad the Titanic. I choose to put the second feature in my model because I believe the bound between parents and children are so tight that it influences people's decision even in a live or death scenario, but the relation between siblings and spouses are less significant in such severe event.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the training data, we can see that there are many missing data, so we need to clean data into more useful ones. \n",
    "\n",
    "# Convert Sex feature from category into data \n",
    "train['Sex'] = train['Sex'].map( {'female':0, 'male':1} ).astype(int)\n",
    "\n",
    "# replace missing age value with median age\n",
    "median_age = train['Age'].dropna().median()\n",
    "if len(train.Age[ train.Age.isnull() ]) > 0:\n",
    "    train.loc[ (train.Age.isnull()), 'Age'] = median_age\n",
    "\n",
    "# replace missing fare value with median fare\n",
    "if len(train.Fare[ train.Fare.isnull() ]) > 0:\n",
    "    train.loc[ (train.Fare.isnull()), 'Fare'] = train.Fare.median()\n",
    "\n",
    "#train data\n",
    "y_train = np.array(train['Survived'])\n",
    "x_train = train.drop([\"Survived\",'Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Embarked',], axis=1).astype(np.float64)\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#Split training data into half and \n",
    "#x = x_train[0:int((x_train.shape[0])/2)]\n",
    "#x2 = x_train[int((x_train.shape[0])/2):]\n",
    "#y = y_train[0:int((y_train.shape[0])/2)]\n",
    "#y2 = y_train[int((y_train.shape[0])/2):]\n",
    "\n",
    "\n",
    "# Do logistic regression\n",
    "\n",
    "logistic = logis.LogisticRegression()\n",
    "theta = logistic.fit(x_train, y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean our test data\n",
    "\n",
    "# Convert Sex feature from category into data \n",
    "test['Sex'] = test['Sex'].map( {'female':0, 'male':1} ).astype(int)\n",
    "\n",
    "# replace missing age value with median age\n",
    "median_age = test['Age'].dropna().median()\n",
    "if len(test.Age[ test.Age.isnull() ]) > 0:\n",
    "    test.loc[ (test.Age.isnull()), 'Age'] = median_age\n",
    "\n",
    "# replace missing fare value with median fare\n",
    "if len(test.Fare[ test.Fare.isnull() ]) > 0:\n",
    "    test.loc[ (test.Fare.isnull()), 'Fare'] = test.Fare.median()\n",
    "\n",
    "#test data\n",
    "x_PassengerID = np.array(test['PassengerId'])\n",
    "x_test = test.drop(['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Embarked',], axis=1).astype(np.float64)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Fit x_test into our model to give us y\n",
    "y_test = logistic.predict(x_test)\n",
    "\n",
    "#write output data\n",
    "\n",
    "predictions_file = open(\"logistic_titanic.csv\", \"w\")\n",
    "predictions_file_object = csv.writer(predictions_file)\n",
    "predictions_file_object.writerow(['PassengerId', 'Survived'])\t# write the column headers\n",
    "for i in range(0,len(y_test)):# For each row in test file,\n",
    "    predictions_file_object.writerow([str(x_PassengerID[i]), str(y_test[i])])\t\n",
    "\n",
    "## close the file\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
